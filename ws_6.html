<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Workshop: Data science with R (ZEW)</title>
    <meta charset="utf-8" />
    <meta name="author" content="Obryan Poyser" />
    <meta name="date" content="2019-04-04" />
    <link rel="stylesheet" href="css/zew-fonts.css" type="text/css" />
    <link rel="stylesheet" href="css/zew.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Workshop: Data science with R (ZEW)
## Session #6: Unsupervised learning
### Obryan Poyser
### 2019-04-04

---







# Outline

1. Unsupervised learning
    1. Types
    1. Challenges
1. Clustering
    1. (Dis)similiarity measures
        1. Quantitative and non-quantitative
    1. K-means
    1. Hierarchical clustering
1. Dimensionality reduction
    1. Principal components analysis



---
# End-to-end machine learning project


.middle[
1- Look at the big picture.  
2- Get the data.  
3- Discover and visualize the data to gain insights.  
4- Prepare the data for Machine Learning algorithms.
.speccolor1[5- Select a model and train it.  
6- Fine-tune your model.  
7- Present your solution.  
8- Launch, monitor, and maintain your system]]

---
# Unsupervised learning

### Definition

1. Subsumes all kinds of machine learning where there is **no known output**
1. "Learning without a teacher"

--

### Types

1. Unsupervised transformations (UT): algorithms that create a new representation of high dimensional data, which output is potentially easier for other algorithms as well as human to understand the underlying structure.
    1. Dimensionality reduction is the most common application of UT. It summarises the data by creating a smaller feature space that contains as much as possible the variance of the original representation.
    1. Clustering algorithms: partition of data into different groups according to a similarity rule.
    
--

### Challenges

1. Evaluate if the algorithm is learning something salient
1. With unlabeled data is hard to tell if we have done right.
    1. In psychological studies is common to construct factors from a set of features.
1. Is considered as "pre-processing" the data which is going to be used for supervised learning models.
1. Unsupervised learning algorithms are **highly sensitive to scales**.


---
# Unsupervised learning: Clustering (data segmentation)

### Definition

1. The process of grouping similar objects together.
1. Grouping/segmenting a collection of objects into subsets or "clusters", such that those within each cluster are more closely related to one another than objects assigned to different clusters.

--

### Inputs

1. Similarity-based clustering `\((N \times N)\)`
    1. Instrument are typically dissimilarity or distance matrices.
1. Feature-based clustering `\((N \times X_p)\)`

--

### Outputs

1. Flat clustering: partition into disjoint sets
    1. It is necessary to provide ex-ante the number of sets
1. Hierarchical clustering: nested tree sets
    1. It is not required to specify the number of sets
    
---
# Unsupervised learning: Clustering (data segmentation)
## Transformations

.pl[
1. As stated, clustering algorithms are sensitive to scales. It is a common practice to transform the features.
    1. Normalize `\(x_i \in (0,1)\)`: `\(\frac{x_i-min(x_i)}{max(x_i)-min(x_i)}\)`
    1. Standardize `\(x_i \sim (0,1)\)`: `\(\frac{x_i-E(x_i)}{\sigma_{x_i}}\)`
    1. Robust scaler: `\(\frac{x_i-Q_1(x_i)}{Q_3(x_i)-Q_1(x_i)}\)`

```r
x1=1*rep(1, 100)+rnorm(100, sd = 2)
x2=4*rep(1, 100)+rnorm(100)
```
   

```r
stdF &lt;- function(x){
    (x-mean(x, na.rm = T))/sd(x, na.rm = T)
}

normF &lt;- function(x){
    (x-min(x, na.rm = T))/(max(x, na.rm = T)-min(x, na.rm = T))
}
```
 
]

.pr[
&lt;img src="ws_6_files/figure-html/unnamed-chunk-5-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]


---
# Unsupervised learning: Clustering (data segmentation)
## Measuring (dis)similarity

1. A clustering method attempts to group the objects based on the definition of similarity supplied to it.
1. Data is defined in terms of proximity between a pair of objects. Proximity can be measured either by affinity (similarities) or lack of it (dissimilarities).
    1. `\(D\)` of size `\(N \times N\)`, where `\(N\)` is the number of objects.
    1. Matrix `\(D\)` is used as an input for the algorithm
    1. Algorithms assume that the matrix `\(D\)` is symmetric
1. A dissimilarity matrix `\(D\)` is a matrix where `\(d_{i,i}=0\)` and `\(d_{i,j}\ge0\)`, that is, it measures the distance between elements `\(i\)` and `\(j\)`.

---
# Unsupervised learning: Clustering (data segmentation)
## Measuring (dis)similarity based on attributes

1. Most often we have measurements `\(x_{ij}\)` for `\(i = 1, 2, . . . , N\)`, on variables `\(j = 1, 2, . . . , p\)` (also called attributes)
1. Step 1: construct pairwise dissimilarities between the observations and used as input.
1. Step 2: We define a dissimilarity `\(d_j(x_{ij}, x_{i'j})\)` between values of the jth attribute.
1. Step 3: Define the measure according to the type of attribute (feature)

### Quantitative

| Measure | Formula |
|-------------|-----------------------------------------------------------------------------------------------------------------------------------------|
| Absolute | `\(d(x_i,x_i')=abs(x_i-x_i')\)` |
| Squared | `\(d(x_i,x_i')=(x_i-x_i')^2\)` |
| Correlation | `\(\frac{\sum_{i=1}^{n} (x_i - \overline{x})(y_i - \overline{y})} {\sqrt{\sum_{i=1}^{n} (x_i - \overline{x})^2(y_i - \overline{y})^2}}\)` |

---
# Unsupervised learning: Clustering (data segmentation)
## Measuring (dis)similarity based on attributes

### Categorical

1. Ordinal: Ordered set of elements eg Likert scales. Error measures for ordinal features are generally defined by replacing their `\(M\)` original values following:

$$
\frac{i-1/2}{M}, \ i=1,\dots,M
$$

1. Categorical: With unordered categorical (nominal) we must assess the degree-of-difference between pairs of values by creating `\(M \times M\)` matrix of distinct elements. Distance is given by `\(L_{rr'}=1\)` if the elements match.

One can create a correlation of categorical variables with **tetrachoric** (binary) and **polychoric** (ordinal) correlations.
[Nice description](http://www.john-uebersax.com/stat/tetra.htm)

In the [appendix](#appendix) I have included more distance measures.


---
# Unsupervised learning: Clustering (data segmentation)
## Algorithms
.middle2[
1. Combinatorial: work directly on the observed data with no direct reference to an underlying probability model
    1. Most popular
    1. Hard to assess the quality of grouping output
1. Mixture modeling: data is an i.i.d sample from some population described by a probability density function.
1. Mode seekers (“bump hunters”) take a nonparametric perspective, attempting to directly estimate distinct modes of the probability density function.
]

---
# Unsupervised learning: Clustering (data segmentation)

## Combinatorial

1. Each observation is uniquely labeled by an integer `\(i \in \{1, · · ·, N\}\)`.
1. The prespecified number of clusters `\(K &lt; N\)` is postulated, and each one is labeled by an integer `\(k \in \{1, . . . , K\}\)`
1. Each observation is assigned to one and only one cluster.
1. Heuristic procedure
1. Popular algorithms 
    1. K-means clustering
    1. Hierarchical clustering

---
# Unsupervised learning: Clustering (data segmentation)

.pl[
## K-means
&lt;img src="img/session_6/k_means.png" width="100%" style="display: block; margin: auto;" /&gt;
]

.pr[
&lt;img src="img/session_6/k_means2.png" width="70%" style="display: block; margin: auto;" /&gt;
]

---
# Unsupervised learning: Clustering (data segmentation)
.pl[
## Hierarchical clustering (HC)

1. K-means demand to establish the number of clusters beforehand. In contrast, hierarchical clustering methods do not require such specifications.
1. With HC the user has to specify a measure of dissimilarity between (disjoint) groups of observations, based on the pairwise dissimilarities among the observations in the two groups.
1. Produces hierarchical representation of the data
]

.pr[
&lt;img src="img/session_6/hier1.png" width="100%" style="display: block; margin: auto;" /&gt;
]

---
# Unsupervised learning: Clustering (data segmentation)

.pl[
## Hierarchical clustering (HC)
1. Agglomerative (bottom-up): it starts by merging two sets of clusters, then repeat the process in the next level.
    1. Step 1: Every observation represents a cluster
    1. Step 2: Cluster merge into a single cluster in the next level
    1. The dissimilarity at each level could be:
        1. Single linkage (SL) or nearest-neighbor
        1. Complete linkage (CL)
        1. Average linkage (AL)
        1. Centroid linkage (CeL)
        1. Ward’s minimum variance method (ward)
]

.pr[
&lt;img src="img/session_6/hier2.png" width="100%" style="display: block; margin: auto;" /&gt;&lt;img src="img/session_6/hier3.png" width="100%" style="display: block; margin: auto;" /&gt;
]

---
# K-means clustering: segmenting the countries according to WDI


```r
wdi &lt;- WDI::WDI(country = "all", indicator = c("unemp"="SL.UEM.TOTL.ZS"
                                                , "secon_enrol"="SE.SEC.ENRR"
                                                , "gov_exp_ed"="SE.XPD.TOTL.GD.ZS"
                                                , "fert_rate"="SP.DYN.TFRT.IN"
                                                , "life_exp"="SP.DYN.LE00.MA.IN"
                                                , "gni_pc"="NY.GNP.PCAP.CD")
                 , start = 2014
                 , end = 2018) %&gt;% 
    as_tibble()
```


```
## # A tibble: 1,320 x 9
##    iso2c country  year unemp secon_enrol gov_exp_ed fert_rate life_exp
##    &lt;chr&gt; &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
##  1 1A    Arab W…  2014 10.3         70.2      NA         3.41     69.1
##  2 1A    Arab W…  2015 10.3         70.6      NA         3.37     69.3
##  3 1A    Arab W…  2016 10.0         70.9      NA         3.33     69.5
##  4 1A    Arab W…  2017  9.93        71.1      NA        NA        NA  
##  5 1A    Arab W…  2018  9.81        NA        NA        NA        NA  
##  6 1W    World    2014  5.44        76.3       4.72      2.46     69.6
##  7 1W    World    2015  5.45        76.5       4.81      2.45     69.8
##  8 1W    World    2016  5.53        76.8      NA         2.44     70.0
##  9 1W    World    2017  5.49        76.6      NA        NA        NA  
## 10 1W    World    2018  5.38        NA        NA        NA        NA  
## # … with 1,310 more rows, and 1 more variable: gni_pc &lt;dbl&gt;
```

---
# K-means clustering: segmenting the countries according to WDI

.pl[

```r
wdi %&gt;% 
  group_by(country) %&gt;% 
  summarise_at(.vars = vars(unemp:gni_pc)
               , .funs = ~mean(., na.rm = T)
               ) %&gt;% 
  naniar::vis_miss()
```
]

.pr[
&lt;img src="ws_6_files/figure-html/unnamed-chunk-12-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]


---
# K-means clustering: segmenting the countries according to WDI


```r
(wdi_case &lt;- wdi %&gt;% 
  group_by(country) %&gt;% 
  summarise_at(.vars = vars(unemp:gni_pc)
               , .funs = ~mean(., na.rm = T)
               ) %&gt;% 
  filter(complete.cases(.)) %&gt;% # filter and keep complete cases only
  mutate(country=str_to_lower(country)) %&gt;% 
  filter(!grepl(country, pattern = "asia|euro|america|demographic|countries|income|ida|oecd|small|africa|world")
         )
 )
```


```
## # A tibble: 116 x 7
##    country     unemp secon_enrol gov_exp_ed fert_rate life_exp gni_pc
##    &lt;chr&gt;       &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;
##  1 afghanistan  8.81        53.6       3.76      4.81     62.0   595 
##  2 albania     15.8         95.8       3.19      1.71     76.2  4392.
##  3 argentina    7.96       107.        5.57      2.31     72.6 12488.
##  4 armenia     18.1         86.0       2.60      1.62     71.1  3985 
##  5 australia    5.78       156.        5.25      1.81     80.4 57708.
##  6 austria      5.65       100.        5.45      1.48     78.8 47335 
##  7 azerbaijan   4.99        91.7       2.83      1.95     68.9  5772.
##  8 bahrain      1.21       102.        2.60      2.06     75.9 21538.
##  9 bangladesh   4.39        66.6       1.54      2.13     70.6  1265 
## 10 barbados    10.5        108.        5.33      1.80     73.3 15365 
## # … with 106 more rows
```

---
# Necessary packages for this session


```r
library(tidyverse)
library(knitr)
library(magrittr)
library(factoextra)
library(cluster)
library(gridExtra)
```



---
# K-means clustering: segmenting the countries according to WDI

.pl[
## Transformations

```r
(wdi_case %&lt;&gt;% 
  mutate_at(.vars = vars(-country), .funs = ~stdF(.))) %&gt;% 
  pivot_longer(cols = -country) %&gt;% 
  ggplot(aes(fill=name, value))+
  geom_density(alpha=.5, col="white")
```

Tip: The build-in function `scale(x)` does the same as the user-defined function `stdF(x)`
]

.pr[
&lt;img src="ws_6_files/figure-html/unnamed-chunk-15-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]

---
# K-means clustering: segmenting the countries according to WDI

.pl[
## Distance


```r
(wdi_case_df &lt;- wdi_case %&gt;%
    tibble::column_to_rownames("country")) %&gt;% 
  get_dist(method = "euclidean") %&gt;% 
  fviz_dist()
```

Tip: The package `factoextra` is a good tool to visualize clusters in R.

]

.pr[
&lt;img src="ws_6_files/figure-html/unnamed-chunk-16-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]

---
# K-means clustering: segmenting the countries according to WDI

.pl[

```r
set.seed(123)
wdi1 &lt;- wdi_case %&gt;%
    sample_n(50) %&gt;% 
    tibble::column_to_rownames("country")  # kmeans does not work well with tibbles (for now)

(kmeans &lt;- c(2:10) %&gt;% 
    enframe(name = "name", value = "centers") %&gt;% 
    dplyr::select(-name) %&gt;% 
    mutate(k_means=map(centers, ~kmeans(wdi1, centers = ., nstart=25)
                       )
           )
  )
```

```
## # A tibble: 9 x 2
##   centers k_means     
##     &lt;int&gt; &lt;list&gt;      
## 1       2 &lt;S3: kmeans&gt;
## 2       3 &lt;S3: kmeans&gt;
## 3       4 &lt;S3: kmeans&gt;
## 4       5 &lt;S3: kmeans&gt;
## 5       6 &lt;S3: kmeans&gt;
## 6       7 &lt;S3: kmeans&gt;
## 7       8 &lt;S3: kmeans&gt;
## 8       9 &lt;S3: kmeans&gt;
## 9      10 &lt;S3: kmeans&gt;
```
]

.pr[

```r
kmeans$k_means[[1]] %&gt;% summary()
```

```
##              Length Class  Mode   
## cluster      50     -none- numeric
## centers      12     -none- numeric
## totss         1     -none- numeric
## withinss      2     -none- numeric
## tot.withinss  1     -none- numeric
## betweenss     1     -none- numeric
## size          2     -none- numeric
## iter          1     -none- numeric
## ifault        1     -none- numeric
```

]

---
# K-means clustering: segmenting the countries according to WDI

.pl[

```r
kmeans %&lt;&gt;% 
  mutate(k_graph=map(k_means
                     , ~fviz_cluster(object = .
                                     , data = wdi1)+
                       labs(title="")
                     )
  )
```

&lt;img src="ws_6_files/figure-html/unnamed-chunk-20-1.png" width="100%" style="display: block; margin: auto;" /&gt;

]

.pr[
&lt;img src="ws_6_files/figure-html/unnamed-chunk-21-1.png" width="100%" style="display: block; margin: auto;" /&gt;

]

---
# K-means clustering: segmenting the countries according to WDI

.pl[

```r
kmeans %&lt;&gt;% 
  mutate(tidy=map(k_means, ~broom::tidy(.)
                  )
         , augment=map(k_means
                       , ~broom::augment(.
                                         , data=wdi1) %&gt;% 
                         dplyr::select(.rownames, .cluster))
         , glance=map(k_means
                      , ~broom::glance(.
                                       , data=wdi1))
         )
```
1. `tidy`: Summarizes the model’s statistical findings (varies across models)
2. `augment`: Add predictions, residuals, and cluster assignments.
3. `glance`: construct a concise one-row summary of the model.
]

.pr[

```r
kmeans$tidy[[1]]
```

```
## # A tibble: 2 x 9
##       x1     x2     x3     x4     x5     x6  size withinss cluster
##    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;fct&gt;  
## 1  0.359  0.715  0.410 -0.587  0.548  0.346    30     93.2 1      
## 2 -0.251 -1.06  -0.421  1.09  -0.842 -0.694    20     74.0 2
```

```r
kmeans$augment[[1]] %&gt;% head(3)
```

```
## # A tibble: 3 x 2
##   .rownames .cluster
##   &lt;chr&gt;     &lt;fct&gt;   
## 1 estonia   1       
## 2 rwanda    2       
## 3 iceland   1
```

```r
kmeans$glance[[1]]
```

```
## # A tibble: 1 x 4
##   totss tot.withinss betweenss  iter
##   &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;
## 1  288.         167.      121.     1
```

]

---
# K-means clustering: segmenting the countries according to WDI

.pl[
## Optimal number of clusters
1. The rule of thumb (also named Elbow Method) for choosing the `\(K\)` amount of clusters is to visualize gaps in the total within variance inside the cluster. 
    1. The total within-cluster sum of square (wss) measures the closeness (minimum as possible)
    1. Alternatives method: Gap statistic, Silhouette.


```r
kmeans %&gt;% 
  unnest(glance) %&gt;% 
  mutate(ratio=betweenss/tot.withinss
         , diff=tsibble::difference(tot.withinss)) %&gt;% 
  ggplot()+
  geom_line(aes(centers, diff), col="blue")+
  geom_line(aes(centers, tot.withinss), col="red")+
  scale_x_continuous(breaks = 1:10)+
  labs(y="Total within variance sum of squares")
```
]

.pr[
&lt;img src="ws_6_files/figure-html/unnamed-chunk-24-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]

---
# Hierarchical clustering: segmenting the countries according to WDI


```r
hclust &lt;- tidyr::expand_grid(aggl=c("ward.D", "ward.D2", "single"
                                             , "complete", "average", "centroid")
                   , k=2:5) %&gt;% 
  mutate(h_clust=map2(.x = aggl, .y = k, .f = ~eclust(x = wdi1, FUNcluster = "hclust"
                                                      , hc_method = .x
                                                      , k=.y
                                                      , hc_metric = "pearson")
                      )
         )
```




```
## # A tibble: 24 x 3
##    aggl        k h_clust     
##    &lt;chr&gt;   &lt;int&gt; &lt;list&gt;      
##  1 ward.D      2 &lt;S3: hclust&gt;
##  2 ward.D      3 &lt;S3: hclust&gt;
##  3 ward.D      4 &lt;S3: hclust&gt;
##  4 ward.D      5 &lt;S3: hclust&gt;
##  5 ward.D2     2 &lt;S3: hclust&gt;
##  6 ward.D2     3 &lt;S3: hclust&gt;
##  7 ward.D2     4 &lt;S3: hclust&gt;
##  8 ward.D2     5 &lt;S3: hclust&gt;
##  9 single      2 &lt;S3: hclust&gt;
## 10 single      3 &lt;S3: hclust&gt;
## # … with 14 more rows
```

---
# Hierarchical clustering: segmenting the countries according to WDI


```r
fviz_dend(hclust$h_clust[[4]], horiz = T)
```

&lt;img src="ws_6_files/figure-html/unnamed-chunk-28-1.png" width="45%" style="display: block; margin: auto;" /&gt;


---
# Hierarchical clustering: segmenting the countries according to WDI

.pl[
### How sensitive are the cluster affinity to the agglomeration measure?


```r
dendextend::tanglegram(hclust$h_clust[[8]], hclust$h_clust[[12]]
                       , lwd=1
                       , highlight_distinct_edges = FALSE
                       , common_subtrees_color_branches = TRUE)
```

Entanglement measures the correspondence between two trees. It goes from 1 (full entanglement) to 0 (no entanglement).


```r
dlist &lt;- dendextend::dendlist(as.dendrogram(hclust$h_clust[[8]])
                              , as.dendrogram(hclust$h_clust[[12]]))
dendextend::entanglement(dlist)
```

```
## [1] 0.3239344
```

]

.pr[
&lt;img src="ws_6_files/figure-html/unnamed-chunk-30-1.png" width="100%" style="display: block; margin: auto;" /&gt;

]



---
# Unsupervised learning: Principal Components

.pl[
1. Principal components are a sequence of projections of the data, mutually uncorrelated and ordered according to the variance.
1. First, it identifies the hyperplane that lies closest to the data, and then it projects the data onto it.
1. Given a set of data `\(\mathbb{R}^p\)`, a PC provides a linear approximation to the data `\((x_1,\dots, x_N)\)`, of all ranks `\(q \le p\)`.
    1. `\(f(\lambda)=\mu + V_q\lambda\)`
    
where: `\(\mu\)` is a location vector in `\(\mathbb{R}^p\)`, `\(V_q\)` a `\(p \times q\)` matrix with `\(q\)` orthogonal unit vector as columns, and `\(\lambda\)` is a `\(q\)` vector of parameters.
]

.pr[
&lt;img src="img/session_6/pca1.png" width="100%" style="display: block; margin: auto;" /&gt;
]

.footnote[
Source\: Friedman, J., Hastie, T., &amp; Tibshirani, R. (2001)
]

---
# Unsupervised learning: Principal Components

.pl[
1. Components are orthogonal to each other, therefore, there is no common variance among the total set.
1. Typically, one applies Singular Value Decomposition (SVD) to decompose the original data in terms of eigenvalue and eigenvectors.

```r
(pca &lt;- prcomp(wdi1)) %&gt;% 
  summary()
```

```
## Importance of components:
##                           PC1    PC2    PC3     PC4     PC5    PC6
## Standard deviation     1.8336 1.1084 0.8048 0.58128 0.43670 0.3287
## Proportion of Variance 0.5723 0.2091 0.1103 0.05751 0.03246 0.0184
## Cumulative Proportion  0.5723 0.7814 0.8916 0.94914 0.98160 1.0000
```

]

.pr[
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="ws_6_files/figure-html/unnamed-chunk-33-1.png" alt="How many factors include?" width="80%" /&gt;
&lt;p class="caption"&gt;How many factors include?&lt;/p&gt;
&lt;/div&gt;
]

---
# Unsupervised learning: Principal Components

.pl[

```r
fviz_pca(pca, col.var = "contrib", repel=T)
```

&lt;img src="ws_6_files/figure-html/unnamed-chunk-34-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]

.pr[

```r
pheatmap::pheatmap(cor(wdi1))
```

&lt;img src="ws_6_files/figure-html/unnamed-chunk-35-1.png" width="70%" style="display: block; margin: auto;" /&gt;

]
---
# Unsupervised learning: Principal Components

.pl[
## Variables

```r
(pca_var &lt;- get_pca_var(pca))
```

```
## Principal Component Analysis Results for variables
##  ===================================================
##   Name       Description                                    
## 1 "$coord"   "Coordinates for the variables"                
## 2 "$cor"     "Correlations between variables and dimensions"
## 3 "$cos2"    "Cos2 for the variables"                       
## 4 "$contrib" "contributions of the variables"
```


```r
pca_var$contrib %&gt;% 
  as.data.frame() %&gt;% 
  tibble::rownames_to_column("var") %&gt;% 
  janitor::clean_names() %&gt;% 
  pivot_longer(cols = dim_1:dim_6) %&gt;% 
  ggplot(aes(name, value, fill=var))+
  geom_col()
```
]

.pr[
&lt;img src="ws_6_files/figure-html/unnamed-chunk-37-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]

---
# Unsupervised learning: Principal Components

.pl[
## Observations

```r
(pca_var &lt;- get_pca_ind(pca))
```

```
## Principal Component Analysis Results for individuals
##  ===================================================
##   Name       Description                       
## 1 "$coord"   "Coordinates for the individuals" 
## 2 "$cos2"    "Cos2 for the individuals"        
## 3 "$contrib" "contributions of the individuals"
```


```r
pca_var$contrib %&gt;% 
    as.data.frame() %&gt;% 
    tibble::rownames_to_column("obs") %&gt;% 
    janitor::clean_names() %&gt;% 
    pivot_longer(cols = dim_1:dim_6, names_to = "comp") %&gt;% 
    group_by(comp) %&gt;% 
    top_n(n = 3, wt = value) %&gt;% 
    mutate(rank=rank(desc(value))) %&gt;% 
    ggplot(aes(reorder(obs, value), value))+
    geom_col(aes(fill=value))+
    facet_wrap(~comp, scales = "free")+
    theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none")+
    labs(x="", y="Contribution")
```
]

.pr[
&lt;img src="ws_6_files/figure-html/unnamed-chunk-39-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]


---
name: references
# References

1. Everitt, B. S., Landau, S., Leese, M., &amp; Stahl, D. (2011). Cluster Analysis. Wiley.
1. Géron, A. (2017). Hands-on machine learning with Scikit-Learn and TensorFlow: concepts, tools, and techniques to build intelligent systems. " O'Reilly Media, Inc.".
1. Friedman, J., Hastie, T., &amp; Tibshirani, R. (2001). The elements of statistical learning (Vol. 1, No. 10). New York: Springer series in statistics.
1. Murphy, K. P. (2012). Machine learning: a probabilistic perspective. MIT press.



---
name: appendix
# Appendix

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="img/session_6/binary_dis.png" alt="Counts of binary outcomes for two individuals (source: Everitt et al 2011)" width="50%" /&gt;
&lt;p class="caption"&gt;Counts of binary outcomes for two individuals (source: Everitt et al 2011)&lt;/p&gt;
&lt;/div&gt;

---
# Appendix

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="img/session_6/measure_bin.png" alt="Similarity measures for binary data (source: Everitt et al 2011)" width="50%" /&gt;
&lt;p class="caption"&gt;Similarity measures for binary data (source: Everitt et al 2011)&lt;/p&gt;
&lt;/div&gt;

---
# Appendix

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="img/session_6/measure_con.png" alt="Dissimilarity measures for continuous data (source: Everitt et al 2011)" width="50%" /&gt;
&lt;p class="caption"&gt;Dissimilarity measures for continuous data (source: Everitt et al 2011)&lt;/p&gt;
&lt;/div&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "solarized-dark",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
