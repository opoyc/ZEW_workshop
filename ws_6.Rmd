---
title: "Workshop: Data science with R (ZEW)"
subtitle: "Session #6: Unsupervised learning"
author: "Obryan Poyser"
date: "2019-04-04"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: solarized-dark
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    css:
      - "css/zew-fonts.css"
      - "css/zew.css"
---
```{r, echo=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(fig.align = "center")
library(tidyverse)
library(knitr)
```

```{r, echo=FALSE}
stdF <- function(x){
    (x-mean(x, na.rm = T))/sd(x, na.rm = T)
}

normF <- function(x){
    (x-min(x, na.rm = T))/(max(x, na.rm = T)-min(x, na.rm = T))
}

robF <- function(x){
    (x-quantile(x, na.rm = T, probs = 0.25))/(quantile(x, na.rm = T, probs = 0.25)-quantile(x, na.rm = T, probs = .75))
}
```



# Outline

1. Unsupervised learning
    1. Types
    1. Challenges
1. Clustering
    1. (Dis)similiarity measures
        1. Quantitative and non-quantitative
    1. Algorithms
1. Dimensionality reduction



---
# End-to-end machine learning project


.middle[
1- Look at the big picture.  
2- Get the data.  
3- Discover and visualize the data to gain insights.  
4- Prepare the data for Machine Learning algorithms.
.speccolor1[5- Select a model and train it.  
6- Fine-tune your model.  
7- Present your solution.  
8- Launch, monitor, and maintain your system]]

---
# Unsupervised learning

### Definition

1. Subsumes all kinds of machine learning where there is **no known output**
1. "Learning without a teacher"

--

### Types

1. Unsupervised transformations (UT): algorithms that create new representation of high dimensional data, which output is potentially easier for other algorithms as well as human to understand the underlying structure.
    1. Dimensionality reduction is the most common application of UT. It summarises the data by creating a smaller feature space that contains as much as possible the variance of the original representation.
    1. Clustering algorithms: partition of data into different groups according to a similarity rule.
    
--

### Challenges

1. Evaluate if the algorithm is learning something salient
1. With unlabeled data is hard to tell if the we have done right.
    1. In psychological studies is common to construct factors from a set of features.
1. Is considered as "pre-processing" the data which is going to be used for supervised learning models.
1. Unsupervised learning algorithms are **highly sensitive to scales**.


---
# Unsupervised learning: Clustering (data segmentation)

### Definition

1. The process of grouping similar objects together.
1. Grouping/segmenting a collection of objects into subsets or "clusters", such that those within each cluster are more closely related to one another than objects assigned to different clusters.

--

### Inputs

1. Similarity-based clustering $(N \times N)$
    1. Instrument are typically dissimilarity or distance matrices.
1. Feature-based clustering $(N \times X_p)$

--

### Outputs

1. Flat clustering: partition into disjoint sets
    1. It is necessary to provide ex-ante the number of sets
1. Hierarchical clustering: nested tree sets
    1. It is not required to specify the number of sets
    
---
# Unsupervised learning: Clustering (data segmentation)
## Transformations

.pl[
1. As stated, clustering algorithms are sensitive to scales. It is a common practice to transforma the features.
    1. Normalize $x_i \in (0,1)$: $\frac{x_i-min(x_i)}{max(x_i)-min(x_i)}$
    1. Standardize $x_i \sim (0,1)$: $\frac{x_i-E(x_i)}{\sigma_{x_i}}$
    1. Robust scaler: $\frac{x_i-Q_1(x_i)}{Q_3(x_i)-Q_1(x_i)}$
```{r, eval=FALSE}
x1=1*rep(1, 100)+rnorm(100, sd = 2)
x2=4*rep(1, 100)+rnorm(100)
```
    
]

.pr[
```{r, echo=FALSE, out.width="100%", fig.height=5, fig.retina=2}
tibble::tibble(x1=1*rep(1, 100)+rnorm(100, sd = 2)
               ,x2=4*rep(1, 100)+rnorm(100)
               , index=(1:100)) %>% 
    mutate_at(vars(x1, x2), .funs = list(std=~stdF(.)
                                         , norm=~normF(.)
                                         , rob=~robF(.))) %>%
    pivot_longer(cols = -index) %>% 
    mutate(type=case_when(
        str_detect(name, pattern = "std")~"std"
        , str_detect(name, pattern = "norm")~"norm"
        , str_detect(name, pattern = "rob")~"robust"
        , TRUE~"original"
    )
    , var=str_sub(name, 1, 2)
    ) %>% 
    ggplot()+
    geom_boxplot(aes(var, value, fill=type))+
    theme(legend.position = "bottom")
```
]


---
# Unsupervised learning: Clustering (data segmentation)
## Measuring (dis)similarity

1. A clustering method attempts to group the objects based on the definition of similarity supplied to it.
1. Data is defined in terms of proximity between a pair of objects. Proximity can be measure either by affinity (similarities) or lack of it (dissimilarities).
    1. $D$ of size $N \times N$, where $N$ is the number of objects.
    1. Matrix $D$ is used as an input for the algorithm
    1. Algorithms assumes that the matrix $D$ is simmetric
1. A dissimilarity matrix $D$ is a matrix where $d_{i,i}=0$ and $d_{i,j}\ge0$, that is, it measures the distance between elements $i$ and $j$.

---
# Unsupervised learning: Clustering (data segmentation)
## Measuring (dis)similarity based on attributes

1. Most often we have measurements $x_{ij}$ for $i = 1, 2, . . . , N$, on variables $j = 1, 2, . . . , p$ (also called attributes)
1. Step 1: construct pairwise dissimilarities between the observations and used as input.
1. Step 2: We define a dissimilarity $d_j(x_{ij}, x_{i'j})$ between values of the jth attribute.
1. Step 3: Define the measure according to the type of attribute (feature)

### Quantitative

| Measure | Formula |
|-------------|-----------------------------------------------------------------------------------------------------------------------------------------|
| Absolute | $d(x_i,x_i')=abs(x_i-x_i')$ |
| Squared | $d(x_i,x_i')=(x_i-x_i')^2$ |
| Correlation | $\frac{\sum_{i=1}^{n} (x_i - \overline{x})(y_i - \overline{y})} {\sqrt{\sum_{i=1}^{n} (x_i - \overline{x})^2(y_i - \overline{y})^2}}$ |

---
# Unsupervised learning: Clustering (data segmentation)
## Measuring (dis)similarity based on attributes

### Categorical

1. Ordinal: Ordered set of elements eg likert scales. Error measures for ordinal features are generally defined by replacing their $M$ original values following:

$$
\frac{i-1/2}{M}, \ i=1,\dots,M
$$

1. Categorical: With unordered categorical (nomimal) we must assess the degree-of-difference between pairs of values by creating $M \times M$ matrix of distinct elements. Distance is given by $L_{rr'}=1$ if the elements match.

One can create a correlation of categorical variables with **tetrachoric** (binary) and **polychoric** (ordinal) correlations.
[Nice description](http://www.john-uebersax.com/stat/tetra.htm)

In the [appendix](#appendix) I have included more distance measures.


---
# Unsupervised learning: Clustering (data segmentation)
## Algorithms
.middle2[
1. Combinatorial: work directly on the observed data with no direct reference to an underlying probability model
    1. Most popular
    1. Hard to assess the quality of grouping output
1. Mixture modeling: data is an i.i.d sample from some population described by a probability density function.
1. Mode seekers (“bump hunters”) take a nonparametric perspective, attempting to directly estimate distinct modes of the probability density function.
]

---
# Unsupervised learning: Clustering (data segmentation)

## Combinatorial

1. Each observation is uniquely labeled by an integer $i \in \{1, · · ·, N\}$.
1. Prespecified number of clusters $K < N$ is postulated, and each one is labeled by an integer $k \in \{1, . . . , K\}$
1. Each observation is assigned to one and only one cluster.
1. Heuristic procedure
1. Popular algorithms 
    1. K-means clustering
    1. Hierarchical clustering

---
# Unsupervised learning: Clustering (data segmentation)

.pl[
## K-means
```{r, echo=FALSE, out.width="100%"}
include_graphics("img/session_6/k_means.png")
```
]

.pr[
```{r, echo=FALSE, out.width="70%"}
include_graphics("img/session_6/k_means2.png")
```
]

---
# Unsupervised learning: Clustering (data segmentation)
.pl[
## Hierarchical clustering (HC)

1. K-means demand to establish the number of clusters beforehand. In contrast, hierarchical clustering methods do no require such specifications.
1. With HC the user have to specify a measure of dissimilarity between (disjoint) groups of observations, based on the pairwise dissimilarities among the observations in the two groups.
1. Produces hierarchical reprentation of the data
]

.pr[
```{r, echo=FALSE}
include_graphics("img/session_6/hier1.png")
```
]

---
# Unsupervised learning: Clustering (data segmentation)

.pl[
## Hierarchical clustering (HC)
1. Agglomerative (bottom-up): it starts by merging two set of clusters, then repeat the process in the next level.
    1. Step 1: Every observation represents a cluster
    1. Step 2: Cluster merge into a single cluster in the next level
    1. The dissimilarity at each level could be:
        1. Single linkage (SL) or nearest-neighbor: closest intergroup similarity (least dissimilar)
        1. Complete linkage (CL): furthest intergroup dissimilarity (most dissimilar)
        1. Group average (GA): average dissimilarity between groups.
]

.pr[
```{r, echo=FALSE}
include_graphics("img/session_6/hier2.png")
include_graphics("img/session_6/hier3.png")
```
]

---
# Unsupervised learning: Clustering (data segmentation)
## Study case: World Development Indicators

```{r, eval=FALSE}
wdi <- WDI::WDI(country = "all", indicator = c("unemp"="SL.UEM.TOTL.ZS"
                                                , "secon_enrol"="SE.SEC.ENRR"
                                                , "gov_exp_ed"="SE.XPD.TOTL.GD.ZS"
                                                , "fert_rate"="SP.DYN.TFRT.IN"
                                                , "life_exp"="SP.DYN.LE00.MA.IN"
                                                , "gni_pc"="NY.GNP.PCAP.CD")
                 , start = 2014
                 , end = 2018) %>% 
    as_tibble()
```

```{r, echo=FALSE}
(wdi <- readr::read_rds("datasets/wdi.rds"))
```

---
# Unsupervised learning: Clustering (data segmentation)
## Study case: World Development Indicators

```{r step1, eval=FALSE}
wdi %>% 
  group_by(country) %>% 
  summarise_at(.vars = vars(unemp:gni_pc)
               , .funs = ~mean(., na.rm = T)
               )
```

```{r ref.label="step1", echo=FALSE}

```

---
# Unsupervised learning: Clustering (data segmentation)
## Study case: World Development Indicators

```{r step2, eval=FALSE}
wdi %>% 
  group_by(country) %>% 
  summarise_at(.vars = vars(unemp:gni_pc)
               , .funs = ~mean(., na.rm = T)
               ) %>% 
  filter(complete.cases(.)) # filter and keep complete cases only
```

```{r ref.label="step2", echo=FALSE}

```











---
name: references
# References

1. Everitt, B. S., Landau, S., Leese, M., & Stahl, D. (2011). Cluster Analysis. Wiley.
1. Géron, A. (2017). Hands-on machine learning with Scikit-Learn and TensorFlow: concepts, tools, and techniques to build intelligent systems. " O'Reilly Media, Inc.".
1. Friedman, J., Hastie, T., & Tibshirani, R. (2001). The elements of statistical learning (Vol. 1, No. 10). New York: Springer series in statistics.
1. Murphy, K. P. (2012). Machine learning: a probabilistic perspective. MIT press.



---
name: appendix
# Appendix

```{r, echo=F, fig.cap="Counts of binary outcomes for two individuals (source: Everitt et al 2011)", out.width="50%"}
include_graphics("img/session_6/binary_dis.png")
```

---
# Appendix

```{r, echo=F, fig.cap="Similarity measures for binary data (source: Everitt et al 2011)", out.width="50%"}
include_graphics("img/session_6/measure_bin.png")
```

---
# Appendix

```{r, echo=F, fig.cap="Dissimilarity measures for continuous data (source: Everitt et al 2011)", out.width="50%"}
include_graphics("img/session_6/measure_con.png")
```





