<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Workshop: Data science with R (ZEW)</title>
    <meta charset="utf-8" />
    <meta name="author" content="Obryan Poyser" />
    <meta name="date" content="2019-04-18" />
    <link rel="stylesheet" href="css/zew-fonts.css" type="text/css" />
    <link rel="stylesheet" href="css/zew.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Workshop: Data science with R (ZEW)
## Session #7: Supervised Learning
### Obryan Poyser
### 2019-04-18

---




# Outline

- What is Supervised Learning?
- Machine Learning vs Econometrics
  - Prediction/classification vs inference
- Parametrics vs Non-parametric estimation
- Classification
  - Logistic (softmax)
  - Performance measures
- Prediction
  - Regularization, shrinkage and variable selection
    - Sparsity, Ridge and LASSO regressions
  - Ensemble learning
  - Performance measures
---
# What is supervised learning?

- Broadly speaking the main two subfields of machine learning are supervised learning and unsupervised learning.
- Supervised statistical involves building statistical models `\(f()\)` for **predicting** or **estimating** and *output* `\((Y)\)` based on one or more *inputs* given by the design matrix `\((X)\)`.
$$
\hat{Y}=\hat{f}(X)
$$
---
# Why estimate `\(f\)`?

.pl[
## Prediction &amp; classification

- In classification the goal is to predict a *class label* within a defined set of elements.
- Prediction is mostly associated with continous data.
- `\(\hat{f}\)` could be treated as a *"black box"*, that is, we are not concerned on the form of `\(\hat{f}\)`, instead, how good this function predicts `\(\hat{Y}\)`.
- No matter how accurate `\(\hat{f}\)` is (by choosing a statistical learning technique), there always be non reducible error term (irreducible error).
- We can improve `\(\hat{f}\)` by choosing a more flexible form. But it has a cost!
]


.pr[
## Inference

- We want to understand the effect of `\(X_1,\dots, X_p\)` on `\(Y\)`.
- By definition it can not be a *"black box"* function, because we need to know the exact form.
- ML statistics differentiates from Econometrics in this element.
  - Econometrics: *"the quantitative analysis of actual economic phenomena based on the concurrent development of theory and observation, related by appropriate methods of inference.*" [source](https://www.jstor.org/stable/1907538?seq=1#metadata_info_tab_contents)
  - Machine learning: *"Machine Learning is the field of study that gives computers the ability to learn without being explicitly programmed"* Arthur Samuel, 1959.
- Causal inference is getting attention from ML supporters, but there is much work to do.
]

.footnote[
The use of `\(\hat{}\)` denote estimates.
]

---
# Econometrics vs Machine Learning: the `\(\hat{\beta}\)` vs `\(\hat{y}\)` dilemma

.pl[
- Many economic applications revolve around *parameter estimation*
    - Produce good estimates that unveil the true relationship between `\(y\)` and `\(X\)`
- Machine learning algorithms are not designed for estimating purposes
    - One has to be aware of the properties and goals of the estimators, the typical parameters' interpretation (i.e. asymptotic theory least square) is no neccesary longer valid.
- Applications
    - New data for traditional questions: for instance: measuring the level of economic activity from  satellite maps using light-intensity measures.
    - Pre-processing
        - Propensity Score Matching, Linear instrumental Variables Regression, Heterogeneous treatment effects.
- ML algorithms are technically easy to use in Python or R
    - Threats: *naive interpretations!*
]

.pr[
&lt;img src="img/session_7/fight.png" width="100%" style="display: block; margin: auto;" /&gt;
]

---
# Predicting poverty from satellite maps

&lt;iframe width="840" align="center" height="472" src="https://www.youtube.com/embed/DafZSeIGLNE?start=92" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;

.footnote[
Sustainability and artificial intelligence lab (Stanford University)
]

---
# How to we estimate `\(\hat{f}\)`?

.pl[
## Parametric methods

- There is a reason for imposing a function form of `\(f\)`: interpretability. In econometrics the conditional expectation function `\(E(Y|X=x)=f(x)\)`, we restrict `\(f\)` from a theoretical grounds. Nonetheless, the CEF is by default nonparametric.
- Generalized Linear Model (GLMs) assume the design matrix can be expressed as a linear combination of a set of parameters (tipically denoted as `\(\beta\)`) obtained by OLS. The estimate parameters are linear by construction, while the `\(X=x_i\)` can be of any form or distribution, however, some transformation may be captured easier.
Example: `\(income=\beta_0+\beta_1 education + \beta_2 seniority\)`
]

.pr[
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="img/session_7/linear.png" alt="Parametric Linear Model. Hastie et al. 2013." width="100%" /&gt;
&lt;p class="caption"&gt;Parametric Linear Model. Hastie et al. 2013.&lt;/p&gt;
&lt;/div&gt;
]

---
# How to we estimate `\(\hat{f}\)`?

.pl[
## Non-parametric methods

- Non-parametric methods do not make explicit assumptions about the functional form of `\(f\)`.
- Avoiding the assumption of a particular functional form for f, they have the potential to accurately fit a wider range of possible shapes for f.
- Disadvantage: the amount of parameters increases drastically!
- Examples:
  - Multiple Adaptive Regression Splines: `\(\hat{f}(X)=\sum_{j=1}^k c_iB_i(X_j)\)`
  - Generalized Additive Models:  `\(\hat{f}(X)=s_0+\sum_{j=1}^p s_j(X_j)\)`
  - Random Forest
  - Neural Network
  - ...
]

.pr[
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="img/session_7/splines.png" alt="Non-parametric model. Hastie et al. 2013." width="100%" /&gt;
&lt;p class="caption"&gt;Non-parametric model. Hastie et al. 2013.&lt;/p&gt;
&lt;/div&gt;
]

---
# Trade-offs of selecting `\(f\)`

.pl[
## Interpretation wise
- The more complex (flexible) the function form, the best it will fit the design matrix data gererating process.
- The simplest a functional form is, the more interpretable are their parameters (GLM for instance), but, its fit is generally worse.

## Generalization wise
- Complex models tend to do a good job explaning the data used to estimate `\(f\)`, but it does very bad job explaining data not used in the estimation process, this is called **overfitting**.
- Complex models oftenly have **hyperparameters**, also name smoothing or complexity parameters, that cannot be estimate from the data. Examples: penalty term, width of the kernel.
- **Underfitting** happens when the model is too simple,
]

.pr[
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="img/session_7/complexity.png" alt="Müller, Guido (2017)" width="100%" /&gt;
&lt;p class="caption"&gt;Müller, Guido (2017)&lt;/p&gt;
&lt;/div&gt;
]

---
# Accuracy

- In order to evaluate the performance of a statistical learning method on a given data set, we need some way to measure how well its predictions actually match the observed data.
- We are interested in the accuracy of the *unseen* data, that is, a subsample of information which has not been used to estimate `\(\hat{f}\)`.
  - In practice, one has to divide the total number of observation into **training** and **testing** subsets. The firs step is to use the **train** data and observe how good it predicts the **test** data.
  - As mentioned, complex models do a good job finding a function that captures as much variance for training design matrix, but they tend to extrapolate badly on **new information**, which make them useless.
- The fundamental problem of selecting `\(f\)` is the trade off between **bias** and **variance**.

$$
E(Y-\hat{f}(X))^2=Var(\hat{f}(X))+[Bias(\hat{f}(X))]^2+Var(\epsilon)
$$

- Variance refers to the amount by which f ˆ would change if we estimated it using a different training data set. 
- Bias refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. 


---
# Bias variance trade-off

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="img/session_7/bv.png" alt="James et al. 2013" width="60%" /&gt;
&lt;p class="caption"&gt;James et al. 2013&lt;/p&gt;
&lt;/div&gt;

---
# Lasso and Ridge regression

.pl[
- Least Square estimates present oftenly two inconvenients
  - Low prediction accuracy: low bias but large variance.
  - Interpretation: With a lower set of predictors is easier to visualize the *"big picture"*.
  - Subset selection can be applied following *(for)backward-stepwise selection ad-hoc methods*.
- Another alternative is to use shrinkage (regularization) techniques that penalized parameter space, and reduce the sparcity of design matrix, that is, when `\(N&lt;X\)`.
- The goal is to reduce the space of regressors by imposing restrictions. Examples:
  - Spike and slab: In Bayesian models it is possible to impose a strong prior to shrinkage the parameter space.
  - Principal component regression
  - **Ridge and Lasso**
]

.pr[
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="img/session_7/spikeslab.png" alt="Spike and Slab priors. (Poyser, 2018)" width="100%" /&gt;
&lt;p class="caption"&gt;Spike and Slab priors. (Poyser, 2018)&lt;/p&gt;
&lt;/div&gt;
]

---

.pl[
# Ridge regression
- Shrinks the regression coefficients by imposing a penalty on their size
- The ridge coefficients minimize a penalized residual sum of squares

$$
`\begin{align}
\hat{\beta}^{ridge}=&amp;\underset{\beta}{\arg\max} \Big\{ \sum_{i=1}^N \Big(y_i-\beta_0-\sum_{j=1}^p x_{ij}\beta_j\Big)^2 + \lambda \sum_{j=1}^p\beta_j^2 \Big\} \\
  =&amp;(X^TX+\lambda I)^{-1}X^Ty
\end{align}`
$$

Here `\(\lambda \geq 0\)` is the tuning parameter and controls the complexity of the estimation. The greater `\(\lambda\)` the greater the shrinkage.
- As expected, scaling affects the shrinkage component, therefore is neccesary to standardize the inputs.
  - **Coefficients are no longer scale invariant**
- Ridge uses the so-called `\(L_2\)` norm
]

.pr[
# Lasso regression

- Similar to Ridge, but instead of shrinking the coefficients with a squared rule of the the parameters, it imposes a absolute value
- The penalty is a `\(L_1\)` norm `\(||\)`
- Solution in `\(y_i\)` is non-linear
- Lasso does a continuous subset selection

$$
`\begin{align}
\hat{\beta}^{lasso}=&amp;\underset{\beta}{\arg\max} \Big\{ \sum_{i=1}^N \Big(y_i-\beta_0-\sum_{j=1}^p x_{ij}\beta_j\Big)^2 + \lambda \sum_{j=1}^p |\beta_j| \Big\}
\end{align}`
$$
- Lasso `\(\lambda\)` shrinkage paramets is chosen with the goal of minimizen the expected prediction error.

]

---
# Decision-tree based models

.pl[

- Decision trees are widely used models for classification and regression tasks.
- Essentially, they learn a hierarchy of if/else questions, leading to a decision.
- Tree-based methods are simple and useful for interpretation. However, they typically are not competitive with the best supervised learning approaches
- `\(R_1\)`, `\(R_2\)`, `\(R_3\)` are known as *leaves* or *terminal nodes* of the tree
- Steps:
  - Dividing the predictor space `\(X_p\)` into `\(R_1, \dots, R_J\)` non-overlapping regions
  - For every observation that falls into `\(R_J\)`, take the mean of the dependent variable
- The objective is to minimize the Residual Sums of Squares of:
`\begin{equation}
\sum_{j=1}^J \sum_{i \in R_j} (y_i-\hat{y}_{R_j})^2
\end{equation}`

]

.pr[
&lt;img src="img/session_7/tree.png" width="50%" style="display: block; margin: auto;" /&gt;
]

---
# Bagging

- Decision trees have the advantage of interpretability, however, they tend show high variance
- One way to reduce the variance is to take the average of all the predictions
- Since the actual design matrix does not change, it is not possible to take the average of inexistent samples. To solve this problem we can use **bootstrapping**
  - What bootstrap does is taking `\(B\)` random samples with replacement from a single sample, creating several artificial samples.
- Bagging uses a boostrap rule to create seudo-training sets, then average the prediction in order to reduce the variance.

`\begin{equation}
\hat{f}_{bag}(x)=\frac{1}{B}\sum_{b=1}^B \hat{f}^{*b}(x)
\end{equation}`

- Eventhough bagging improves accuracy, it reduces interpretability. Nevertheless, there is one way to assess the importance of one variable: variable importance
- The process works as follows:
  - Take one variable at a time, then calculate how much the RSS decreases in each split

---
# Bagging

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="img/session_7/bagging.png" alt="Bagging process. Géron (2017)" width="70%" /&gt;
&lt;p class="caption"&gt;Bagging process. Géron (2017)&lt;/p&gt;
&lt;/div&gt;

---
# Random-forest

- Works similar as bagging, but with the difference of taking a random sample of predictors over the design matrix `\(X_p\)`
- Process:
  - Draw a bootstrap sample `\(Z^∗\)` of size `\(N\)` from the training data.
  - Grow a random-forest tree `\(T_b\)` to the bootstrapped data, by recursively repeating the following steps for each terminal node of the tree, until the minimum node size `\(n_min\)` is reached.
    - Select `\(m\)` variables at random from the `\(p\)` variables.
    - Pick the best variable/split-point among the `\(m\)`.
    - Split the node into two daughter nodes.

## Considerations

- There are several algorithms and tweaks to improve prediction accuracy, however, each of them deserves a whole session to be studied.
  - Boosting
  - Ensemble
  - Boosting
  - SVM
  - Neural Networks
  - ...
- For a more detailed explanation of such techniques see the [references](#references) section.

---
# Packages

- `broom`: is an attempt to bridge the gap from untidy outputs of predictions and estimations to the tidy data we want to work with.
    - `tidy`: constructs a data frame that summarizes the model’s statistical findings. This includes coefficients and p-values for each term in a regression, per-cluster information in clustering applications, or per-test information for multtest functions.
    - `augment`: add columns to the original data that was modeled. This includes predictions, residuals, and cluster assignments.
    - `glance`: construct a concise one-row summary of the model. This typically contains values such as R^2, adjusted R^2, and residual standard error that are computed once for the entire model.

- [`parsnip`](https://tidymodels.github.io/parsnip/articles/articles/Models.html)
  - It is designed to solve a specific problem related to model fitting in R, **the interface**. 
  -  Many functions have different interfaces and arguments names and parsnip standardizes the interface for fitting models as well as the return values

- `tidymodels`: Collection of modeling packages, that aim to create a common structure (similar to sklearn in Python)
- `rsample`: Classes and functions to create and summarize different types of resampling objects (e.g. bootstrap, cross-validation).
---
# Dataset

## Wine quality

```r
wine_w &lt;- readr::read_rds("datasets/session_7/wine_white.rds")
skimr::skim(wine_w) %&gt;% skimr::kable()
```

```
## Skim summary statistics  
##  n obs: 4898    
##  n variables: 13    
## 
## Variable type: numeric
## 
##        variable          missing    complete     n       mean      sd       p0       p25      p50     p75     p100      hist   
## ----------------------  ---------  ----------  ------  --------  -------  -------  -------  -------  ------  ------  ----------
##        alcohol              0         4898      4898    10.51     1.23       8       9.5     10.4     11.4    14.2    ▁▇▆▆▃▃▂▁ 
##       chlorides             0         4898      4898    0.046     0.022    0.009    0.036    0.043    0.05    0.35    ▇▂▁▁▁▁▁▁ 
##      citric_acid            0         4898      4898     0.33     0.12       0      0.27     0.32     0.39    1.66    ▁▇▂▁▁▁▁▁ 
##        density              0         4898      4898     0.99     0.003    0.99     0.99     0.99      1      1.04    ▇▇▁▁▁▁▁▁ 
##     fixed_acidity           0         4898      4898     6.85     0.84      3.8      6.3      6.8     7.3     14.2    ▁▃▇▂▁▁▁▁ 
##  free_sulfur_dioxide        0         4898      4898    35.31     17.01      2       23       34       46     289     ▇▅▁▁▁▁▁▁ 
##          p_h                0         4898      4898     3.19     0.15     2.72     3.09     3.18     3.28    3.82    ▁▂▆▇▃▂▁▁ 
##        quality              0         4898      4898     5.88     0.89       3        5        6       6       9      ▁▁▅▇▁▃▁▁ 
##       quality_b             0         4898      4898     0.67     0.47       0        0        1       1       1      ▅▁▁▁▁▁▁▇ 
##     residual_sugar          0         4898      4898     6.39     5.07      0.6      1.7      5.2     9.9     65.8    ▇▃▁▁▁▁▁▁ 
##       sulphates             0         4898      4898     0.49     0.11     0.22     0.41     0.47     0.55    1.08    ▁▆▇▃▁▁▁▁ 
##  total_sulfur_dioxide       0         4898      4898    138.36    42.5       9       108      134     167     440     ▁▆▇▃▁▁▁▁ 
##    volatile_acidity         0         4898      4898     0.28      0.1     0.08     0.21     0.26     0.32    1.1     ▃▇▂▁▁▁▁▁
```

---
# Tranining, testing

&lt;img src="img/session_7/split1.png" width="60%" style="display: block; margin: auto;" /&gt;

---
# Cross-validation

&lt;img src="img/session_7/split2.png" width="60%" style="display: block; margin: auto;" /&gt;

---
# Tranining, testing, and cross-validation


```r
data_split &lt;- initial_split(wine_w, prop = .7)
(train_data &lt;- training(data_split)) %&gt;% dim()
```

```
## [1] 3429   13
```

```r
(test_data &lt;- testing(data_split)) %&gt;% dim()
```

```
## [1] 1469   13
```

---
# Cross-validation


```r
(cv_data &lt;- vfold_cv(train_data, v = 10) %&gt;% 
    mutate(train=map(splits, ~training(.x))
           , validate=map(splits, ~testing(.x))
           , lm=map(train, ~lm(quality~.-quality_b, data = .x))
           , rf=map(train, ~rand_forest(mtry = 5, trees = 200) %&gt;%
                        set_engine("ranger", importance="impurity") %&gt;%
                        fit(quality~.-quality_b, data=.x)
                    )
    )
)
```

```
## #  10-fold cross-validation 
## # A tibble: 10 x 6
##    splits          id     train             validate        lm      rf     
##  * &lt;list&gt;          &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          &lt;list&gt;  &lt;list&gt; 
##  1 &lt;split [3.1K/3… Fold01 &lt;tibble [3,086 ×… &lt;tibble [343 ×… &lt;S3: l… &lt;fit[+…
##  2 &lt;split [3.1K/3… Fold02 &lt;tibble [3,086 ×… &lt;tibble [343 ×… &lt;S3: l… &lt;fit[+…
##  3 &lt;split [3.1K/3… Fold03 &lt;tibble [3,086 ×… &lt;tibble [343 ×… &lt;S3: l… &lt;fit[+…
##  4 &lt;split [3.1K/3… Fold04 &lt;tibble [3,086 ×… &lt;tibble [343 ×… &lt;S3: l… &lt;fit[+…
##  5 &lt;split [3.1K/3… Fold05 &lt;tibble [3,086 ×… &lt;tibble [343 ×… &lt;S3: l… &lt;fit[+…
##  6 &lt;split [3.1K/3… Fold06 &lt;tibble [3,086 ×… &lt;tibble [343 ×… &lt;S3: l… &lt;fit[+…
##  7 &lt;split [3.1K/3… Fold07 &lt;tibble [3,086 ×… &lt;tibble [343 ×… &lt;S3: l… &lt;fit[+…
##  8 &lt;split [3.1K/3… Fold08 &lt;tibble [3,086 ×… &lt;tibble [343 ×… &lt;S3: l… &lt;fit[+…
##  9 &lt;split [3.1K/3… Fold09 &lt;tibble [3,086 ×… &lt;tibble [343 ×… &lt;S3: l… &lt;fit[+…
## 10 &lt;split [3.1K/3… Fold10 &lt;tibble [3,087 ×… &lt;tibble [342 ×… &lt;S3: l… &lt;fit[+…
```

---
# Performance

.pl[
## Linear model


```r
lm_model &lt;- cv_data %&gt;%   
    mutate(pred=map2(.x=lm, .y=validate, ~predict(.x, .y) %&gt;% 
                         as_tibble %&gt;% 
                         set_names("pred"))
           , mae=map2_dbl(.x=pred, .y=validate
                          , ~measures::MAE(truth = .y$quality
                                           , response = .x$pred)
                          )
           , mse=map2_dbl(.x=pred, .y=validate
                          , ~measures::MSE(truth = .y$quality
                                           , response = .x$pred)
           ))
```
]

.pr[
## Random forest


```r
rf_model &lt;- cv_data %&gt;%   
    mutate(pred=map2(.x=rf, .y=validate, ~predict(.x, .y) %&gt;% 
                         as_tibble %&gt;% 
                         set_names("pred"))
           , mae=map2_dbl(.x=pred, .y=validate
                          , ~measures::MAE(truth = .y$quality
                                           , response = .x$pred)
                          )
           , mse=map2_dbl(.x=pred, .y=validate
                          , ~measures::MSE(truth = .y$quality
                                           , response = .x$pred)
           ))
```
]

---
# Model performance

.pl[

```r
lm_model %&gt;%
    dplyr::select(id, mae, mse) %&gt;% 
    bind_rows(rf_model %&gt;% 
                dplyr::select(id, mae, mse)
              , .id = "model") %&gt;% 
    mutate(model=case_when(
        model==1~"linear"
        , T~"random_forest"
    )) %&gt;%
    pivot_longer(cols = c(mae, mse)
                 , names_to = "measure") %&gt;% 
    ggplot(aes(model, value))+
    geom_boxplot() +
    geom_jitter(aes(col=id))+
    facet_grid(~measure, scales = "free")
```
]

.pr[
&lt;img src="ws_7_files/figure-html/unnamed-chunk-16-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]




---
# Hyperparameter tuning: grid search


```r
(rf_tun &lt;- vfold_cv(train_data, v = 5) %&gt;% 
    crossing(mtry=1:5) %&gt;% 
    mutate(train=map(splits, ~training(.x))
           , validate=map(splits, ~testing(.x))
           , rf=map2(.x=train, .y=mtry, ~rand_forest(mtry = .y, trees = 200) %&gt;%
                        set_engine("ranger", importance="impurity") %&gt;%
                        fit(quality~.-quality_b, data=.x)
           ))
)
```

```
## # A tibble: 25 x 6
##    splits           id     mtry train              validate         rf     
##    &lt;list&gt;           &lt;chr&gt; &lt;int&gt; &lt;list&gt;             &lt;list&gt;           &lt;list&gt; 
##  1 &lt;split [2.7K/68… Fold1     1 &lt;tibble [2,743 × … &lt;tibble [686 × … &lt;fit[+…
##  2 &lt;split [2.7K/68… Fold1     2 &lt;tibble [2,743 × … &lt;tibble [686 × … &lt;fit[+…
##  3 &lt;split [2.7K/68… Fold1     3 &lt;tibble [2,743 × … &lt;tibble [686 × … &lt;fit[+…
##  4 &lt;split [2.7K/68… Fold1     4 &lt;tibble [2,743 × … &lt;tibble [686 × … &lt;fit[+…
##  5 &lt;split [2.7K/68… Fold1     5 &lt;tibble [2,743 × … &lt;tibble [686 × … &lt;fit[+…
##  6 &lt;split [2.7K/68… Fold2     1 &lt;tibble [2,743 × … &lt;tibble [686 × … &lt;fit[+…
##  7 &lt;split [2.7K/68… Fold2     2 &lt;tibble [2,743 × … &lt;tibble [686 × … &lt;fit[+…
##  8 &lt;split [2.7K/68… Fold2     3 &lt;tibble [2,743 × … &lt;tibble [686 × … &lt;fit[+…
##  9 &lt;split [2.7K/68… Fold2     4 &lt;tibble [2,743 × … &lt;tibble [686 × … &lt;fit[+…
## 10 &lt;split [2.7K/68… Fold2     5 &lt;tibble [2,743 × … &lt;tibble [686 × … &lt;fit[+…
## # … with 15 more rows
```

---
# Model performance: post-grid search


```r
rf_model_tun &lt;- rf_tun %&gt;%   
    mutate(pred=map2(.x=rf, .y=validate, ~predict(.x, .y) %&gt;% 
                         as_tibble %&gt;% 
                         set_names("pred"))
           , mae=map2_dbl(.x=pred, .y=validate
                          , ~measures::MAE(truth = .y$quality
                                           , response = .x$pred)
                          )
           , mse=map2_dbl(.x=pred, .y=validate
                          , ~measures::MSE(truth = .y$quality
                                           , response = .x$pred)
           ))
```

---
# Model performance: post-grid search

.pl[

```r
rf_model_tun %&gt;%
    dplyr::select(mtry, id, mae, mse) %&gt;% 
    pivot_longer(cols = c(mae, mse)
                 , names_to = "measure") %&gt;% 
    ggplot(aes(as.factor(mtry), value))+
    geom_boxplot() +
    geom_jitter(aes(col=id))+
    facet_grid(~measure, scales = "free")
```
]

.pr[
&lt;img src="ws_7_files/figure-html/unnamed-chunk-20-1.png" width="100%" style="display: block; margin: auto;" /&gt;

]

---
# Regression: final model


```r
best_model &lt;- rand_forest(mtry = 5, trees = 200) %&gt;%
                        set_engine("ranger", importance="impurity") %&gt;%
                        fit(quality~.-quality_b, data=train_data)
test_pred &lt;- predict(best_model, test_data) %&gt;%
  as_tibble %&gt;%
  set_names("pred")

mae_rf &lt;- measures::MAE(truth = test_data$quality, response = test_pred$pred)
mse_rf &lt;- measures::MSE(truth = test_data$quality, response = test_pred$pred)
```

---
# Ridge

.pl[
## Tweak data

```r
X &lt;- train_data %&gt;% 
  dplyr::select(-quality, -quality_b) %&gt;% as.matrix()
y &lt;- train_data %&gt;%
  dplyr::select(quality) %&gt;% as.matrix()
X_test &lt;- test_data %&gt;% 
  dplyr::select(-quality, -quality_b) %&gt;% as.matrix()
y_test &lt;- test_data %&gt;% 
  dplyr::select(quality) %&gt;% as.matrix()
```

## Model

```r
ridge_cv &lt;- cv.glmnet(X, y, alpha = 0, standardize = T, nfolds = 10)
```
]

.pr[
&lt;img src="ws_7_files/figure-html/unnamed-chunk-23-1.png" width="100%" style="display: block; margin: auto;" /&gt;

]

---
# Ridge

.pl[
## Tweak model

```r
ridge_lambda &lt;- 10^seq(-3, 1, length.out = 100)
ridge_cv_a1 &lt;- cv.glmnet(X, y, alpha = 0, lambda = ridge_lambda
                      , standardize = T, nfolds = 10)
```


```r
ridge_best &lt;- glmnet(X, y, alpha = 0, lambda = ridge_cv_a1$lambda.min
                      , standardize = T)
ridge_pred &lt;- predict(ridge_best, X_test)
mae_ridge &lt;- measures::MAE(truth = test_data$quality, response = ridge_pred)
```
]

.pr[
&lt;img src="ws_7_files/figure-html/unnamed-chunk-26-1.png" width="100%" style="display: block; margin: auto;" /&gt;

]

---
# Ridge

.pl[

```r
ridge_shrink &lt;- glmnet(X, y, alpha = 0
                       , lambda = ridge_lambda
                       , standardize = F)
```
]

.pr[
&lt;img src="ws_7_files/figure-html/unnamed-chunk-28-1.png" width="100%" style="display: block; margin: auto;" /&gt;

]

---
# Lasso

.pl[

```r
lasso_cv &lt;- cv.glmnet(X, y, alpha = 1
                      , standardize = TRUE, nfolds = 10)
lasso_best &lt;- glmnet(X, y, alpha = 1
                     , lambda = lasso_cv$lambda.min
                      , standardize = TRUE)
lasso_pred &lt;- predict(lasso_best, X_test)
mae_lasso &lt;- measures::MAE(truth = test_data$quality
                           , response = lasso_pred)
```
]

.pr[
&lt;img src="ws_7_files/figure-html/unnamed-chunk-30-1.png" width="100%" style="display: block; margin: auto;" /&gt;

]

---
# Lasso shrinkage

.pl[

```r
lasso_lambda &lt;- 10^seq(-3, 1, length.out = 100)
lasso_shrink &lt;- glmnet(X, y, alpha = 1
                       , lambda = lasso_lambda
                       , standardize = F)
```
]

.pr[
&lt;img src="ws_7_files/figure-html/unnamed-chunk-32-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]

---
# Comparison

.pl[

```r
data.frame(linear=mae_lm, random_forest=mae_rf
           , ridge=mae_ridge, lasso=mae_lasso) %&gt;% 
  t() %&gt;%
  as.data.frame() %&gt;% 
  tibble::rownames_to_column("MAE") %&gt;% 
  as_tibble() %&gt;% 
  ggplot(aes(reorder(MAE, desc(V1)), V1))+
  geom_col()+
  labs(x="Algorithm", y="Mean Absolute Error")
```
]
.pr[
&lt;img src="ws_7_files/figure-html/unnamed-chunk-33-1.png" width="100%" style="display: block; margin: auto;" /&gt;

]


---
name: references
# References

- Géron, A. (2017). Hands-On Machine Learning with Scikit-Learn &amp; TensorFlow.
- Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). The Elements of Statistical Learning. Springer, 27(2), 83–85.
- James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013). An Introduction to Statistical Learning. Springer. New York.
- Müller, A. C., &amp; Guido, S. (2017). Introduction to machine learning with Python. O’Reilly.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "solarized-dark",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
